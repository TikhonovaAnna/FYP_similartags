{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing '<python><java>' into a list of ['python','java']\n",
    "def tag_to_taglist(tags):\n",
    "    tag_cleaned = re.sub('[<>]',' ',tags)\n",
    "    tags_list = tag_cleaned.split()\n",
    "    return tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2728: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#def TrainWord2Vec(skipgram,features,windows,minfrequency):\n",
    "    \n",
    "#location of the csv file containing Id, Tags for each post\n",
    "#to be passed into pandas\n",
    "location = 'Id_CleanedTags.csv'\n",
    "#dataframe that contains tags for each post\n",
    "df = pd.read_csv(location, names = ['Tags'])\n",
    "\n",
    "tags_groups = [] #a list of lists, [['python','java'],['c','web']]..\n",
    "for row in df['Tags'][1:]: #print(df['Tags'][0]) --> Tags, starts from index 1\n",
    "    tags_groups.append(tag_to_taglist(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define word2vec parameters\n",
    "num_features = 600 #features\n",
    "min_word_count = 10 #minfrequency\n",
    "num_workers = 2\n",
    "context = 1 #windows\n",
    "downsampling = 1e-3\n",
    "skipgram = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-26 17:15:12,225 : INFO : collecting all words and their counts\n",
      "2018-02-26 17:15:12,227 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-02-26 17:15:12,249 : INFO : PROGRESS: at sentence #10000, processed 29706 words, keeping 3149 word types\n",
      "2018-02-26 17:15:12,273 : INFO : PROGRESS: at sentence #20000, processed 59110 words, keeping 4564 word types\n",
      "2018-02-26 17:15:12,298 : INFO : PROGRESS: at sentence #30000, processed 88611 words, keeping 5606 word types\n",
      "2018-02-26 17:15:12,314 : INFO : PROGRESS: at sentence #40000, processed 117979 words, keeping 6603 word types\n",
      "2018-02-26 17:15:12,333 : INFO : PROGRESS: at sentence #50000, processed 147602 words, keeping 7456 word types\n",
      "2018-02-26 17:15:12,350 : INFO : PROGRESS: at sentence #60000, processed 176933 words, keeping 8205 word types\n",
      "2018-02-26 17:15:12,370 : INFO : PROGRESS: at sentence #70000, processed 206075 words, keeping 8762 word types\n",
      "2018-02-26 17:15:12,389 : INFO : PROGRESS: at sentence #80000, processed 234681 words, keeping 9282 word types\n",
      "2018-02-26 17:15:12,409 : INFO : PROGRESS: at sentence #90000, processed 263716 words, keeping 9780 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-26 17:15:12,428 : INFO : PROGRESS: at sentence #100000, processed 292348 words, keeping 10223 word types\n",
      "2018-02-26 17:15:12,450 : INFO : PROGRESS: at sentence #110000, processed 321176 words, keeping 10691 word types\n",
      "2018-02-26 17:15:12,468 : INFO : PROGRESS: at sentence #120000, processed 350034 words, keeping 11088 word types\n",
      "2018-02-26 17:15:12,486 : INFO : PROGRESS: at sentence #130000, processed 378985 words, keeping 11439 word types\n",
      "2018-02-26 17:15:12,506 : INFO : PROGRESS: at sentence #140000, processed 408468 words, keeping 11806 word types\n",
      "2018-02-26 17:15:12,528 : INFO : PROGRESS: at sentence #150000, processed 437504 words, keeping 12116 word types\n",
      "2018-02-26 17:15:12,543 : INFO : PROGRESS: at sentence #160000, processed 467037 words, keeping 12467 word types\n",
      "2018-02-26 17:15:12,562 : INFO : PROGRESS: at sentence #170000, processed 496178 words, keeping 12756 word types\n",
      "2018-02-26 17:15:12,581 : INFO : PROGRESS: at sentence #180000, processed 525389 words, keeping 13050 word types\n",
      "2018-02-26 17:15:12,599 : INFO : PROGRESS: at sentence #190000, processed 554958 words, keeping 13309 word types\n",
      "2018-02-26 17:15:12,618 : INFO : PROGRESS: at sentence #200000, processed 584558 words, keeping 13589 word types\n",
      "2018-02-26 17:15:12,640 : INFO : PROGRESS: at sentence #210000, processed 614137 words, keeping 13842 word types\n",
      "2018-02-26 17:15:12,658 : INFO : PROGRESS: at sentence #220000, processed 643204 words, keeping 14107 word types\n",
      "2018-02-26 17:15:12,676 : INFO : PROGRESS: at sentence #230000, processed 672277 words, keeping 14357 word types\n",
      "2018-02-26 17:15:12,697 : INFO : PROGRESS: at sentence #240000, processed 701374 words, keeping 14600 word types\n",
      "2018-02-26 17:15:12,719 : INFO : PROGRESS: at sentence #250000, processed 730555 words, keeping 14814 word types\n",
      "2018-02-26 17:15:12,740 : INFO : PROGRESS: at sentence #260000, processed 759767 words, keeping 15009 word types\n",
      "2018-02-26 17:15:12,755 : INFO : PROGRESS: at sentence #270000, processed 789199 words, keeping 15196 word types\n",
      "2018-02-26 17:15:12,775 : INFO : PROGRESS: at sentence #280000, processed 818514 words, keeping 15392 word types\n",
      "2018-02-26 17:15:12,793 : INFO : PROGRESS: at sentence #290000, processed 847755 words, keeping 15591 word types\n",
      "2018-02-26 17:15:12,811 : INFO : PROGRESS: at sentence #300000, processed 877475 words, keeping 15780 word types\n",
      "2018-02-26 17:15:12,831 : INFO : PROGRESS: at sentence #310000, processed 906837 words, keeping 15978 word types\n",
      "2018-02-26 17:15:12,854 : INFO : PROGRESS: at sentence #320000, processed 936105 words, keeping 16171 word types\n",
      "2018-02-26 17:15:12,875 : INFO : PROGRESS: at sentence #330000, processed 965390 words, keeping 16355 word types\n",
      "2018-02-26 17:15:12,896 : INFO : PROGRESS: at sentence #340000, processed 994459 words, keeping 16535 word types\n",
      "2018-02-26 17:15:12,913 : INFO : PROGRESS: at sentence #350000, processed 1023842 words, keeping 16707 word types\n",
      "2018-02-26 17:15:12,935 : INFO : PROGRESS: at sentence #360000, processed 1053352 words, keeping 16858 word types\n",
      "2018-02-26 17:15:12,956 : INFO : PROGRESS: at sentence #370000, processed 1082642 words, keeping 17042 word types\n",
      "2018-02-26 17:15:12,978 : INFO : PROGRESS: at sentence #380000, processed 1112010 words, keeping 17207 word types\n",
      "2018-02-26 17:15:12,998 : INFO : PROGRESS: at sentence #390000, processed 1141540 words, keeping 17381 word types\n",
      "2018-02-26 17:15:13,017 : INFO : PROGRESS: at sentence #400000, processed 1170832 words, keeping 17543 word types\n",
      "2018-02-26 17:15:13,036 : INFO : PROGRESS: at sentence #410000, processed 1200354 words, keeping 17721 word types\n",
      "2018-02-26 17:15:13,063 : INFO : PROGRESS: at sentence #420000, processed 1229714 words, keeping 17909 word types\n",
      "2018-02-26 17:15:13,081 : INFO : PROGRESS: at sentence #430000, processed 1258910 words, keeping 18081 word types\n",
      "2018-02-26 17:15:13,102 : INFO : PROGRESS: at sentence #440000, processed 1288221 words, keeping 18255 word types\n",
      "2018-02-26 17:15:13,124 : INFO : PROGRESS: at sentence #450000, processed 1317644 words, keeping 18428 word types\n",
      "2018-02-26 17:15:13,145 : INFO : PROGRESS: at sentence #460000, processed 1347272 words, keeping 18572 word types\n",
      "2018-02-26 17:15:13,165 : INFO : PROGRESS: at sentence #470000, processed 1376896 words, keeping 18715 word types\n",
      "2018-02-26 17:15:13,187 : INFO : PROGRESS: at sentence #480000, processed 1406332 words, keeping 18871 word types\n",
      "2018-02-26 17:15:13,207 : INFO : PROGRESS: at sentence #490000, processed 1435623 words, keeping 19014 word types\n",
      "2018-02-26 17:15:13,229 : INFO : PROGRESS: at sentence #500000, processed 1465073 words, keeping 19161 word types\n",
      "2018-02-26 17:15:13,259 : INFO : PROGRESS: at sentence #510000, processed 1494532 words, keeping 19338 word types\n",
      "2018-02-26 17:15:13,282 : INFO : PROGRESS: at sentence #520000, processed 1523978 words, keeping 19481 word types\n",
      "2018-02-26 17:15:13,315 : INFO : PROGRESS: at sentence #530000, processed 1553493 words, keeping 19623 word types\n",
      "2018-02-26 17:15:13,335 : INFO : PROGRESS: at sentence #540000, processed 1582877 words, keeping 19761 word types\n",
      "2018-02-26 17:15:13,357 : INFO : PROGRESS: at sentence #550000, processed 1612357 words, keeping 19892 word types\n",
      "2018-02-26 17:15:13,377 : INFO : PROGRESS: at sentence #560000, processed 1641654 words, keeping 20042 word types\n",
      "2018-02-26 17:15:13,393 : INFO : PROGRESS: at sentence #570000, processed 1670749 words, keeping 20175 word types\n",
      "2018-02-26 17:15:13,413 : INFO : PROGRESS: at sentence #580000, processed 1699858 words, keeping 20312 word types\n",
      "2018-02-26 17:15:13,433 : INFO : PROGRESS: at sentence #590000, processed 1729416 words, keeping 20434 word types\n",
      "2018-02-26 17:15:13,453 : INFO : PROGRESS: at sentence #600000, processed 1758874 words, keeping 20563 word types\n",
      "2018-02-26 17:15:13,476 : INFO : PROGRESS: at sentence #610000, processed 1788626 words, keeping 20700 word types\n",
      "2018-02-26 17:15:13,493 : INFO : PROGRESS: at sentence #620000, processed 1818162 words, keeping 20840 word types\n",
      "2018-02-26 17:15:13,509 : INFO : PROGRESS: at sentence #630000, processed 1847617 words, keeping 20963 word types\n",
      "2018-02-26 17:15:13,530 : INFO : PROGRESS: at sentence #640000, processed 1876899 words, keeping 21087 word types\n",
      "2018-02-26 17:15:13,549 : INFO : PROGRESS: at sentence #650000, processed 1905783 words, keeping 21194 word types\n",
      "2018-02-26 17:15:13,570 : INFO : PROGRESS: at sentence #660000, processed 1935126 words, keeping 21329 word types\n",
      "2018-02-26 17:15:13,593 : INFO : PROGRESS: at sentence #670000, processed 1964459 words, keeping 21450 word types\n",
      "2018-02-26 17:15:13,610 : INFO : PROGRESS: at sentence #680000, processed 1993802 words, keeping 21557 word types\n",
      "2018-02-26 17:15:13,629 : INFO : PROGRESS: at sentence #690000, processed 2023010 words, keeping 21665 word types\n",
      "2018-02-26 17:15:13,653 : INFO : PROGRESS: at sentence #700000, processed 2052084 words, keeping 21786 word types\n",
      "2018-02-26 17:15:13,674 : INFO : PROGRESS: at sentence #710000, processed 2081078 words, keeping 21904 word types\n",
      "2018-02-26 17:15:13,691 : INFO : PROGRESS: at sentence #720000, processed 2110846 words, keeping 22042 word types\n",
      "2018-02-26 17:15:13,716 : INFO : PROGRESS: at sentence #730000, processed 2140721 words, keeping 22164 word types\n",
      "2018-02-26 17:15:13,735 : INFO : PROGRESS: at sentence #740000, processed 2170657 words, keeping 22278 word types\n",
      "2018-02-26 17:15:13,756 : INFO : PROGRESS: at sentence #750000, processed 2200885 words, keeping 22398 word types\n",
      "2018-02-26 17:15:13,773 : INFO : PROGRESS: at sentence #760000, processed 2230938 words, keeping 22505 word types\n",
      "2018-02-26 17:15:13,791 : INFO : PROGRESS: at sentence #770000, processed 2261027 words, keeping 22602 word types\n",
      "2018-02-26 17:15:13,815 : INFO : PROGRESS: at sentence #780000, processed 2291239 words, keeping 22732 word types\n",
      "2018-02-26 17:15:13,833 : INFO : PROGRESS: at sentence #790000, processed 2321564 words, keeping 22832 word types\n",
      "2018-02-26 17:15:13,851 : INFO : PROGRESS: at sentence #800000, processed 2351815 words, keeping 22923 word types\n",
      "2018-02-26 17:15:13,871 : INFO : PROGRESS: at sentence #810000, processed 2381970 words, keeping 23034 word types\n",
      "2018-02-26 17:15:13,894 : INFO : PROGRESS: at sentence #820000, processed 2412083 words, keeping 23157 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-26 17:15:13,921 : INFO : PROGRESS: at sentence #830000, processed 2442266 words, keeping 23262 word types\n",
      "2018-02-26 17:15:13,946 : INFO : PROGRESS: at sentence #840000, processed 2472293 words, keeping 23368 word types\n",
      "2018-02-26 17:15:13,962 : INFO : PROGRESS: at sentence #850000, processed 2502480 words, keeping 23482 word types\n",
      "2018-02-26 17:15:13,982 : INFO : PROGRESS: at sentence #860000, processed 2532430 words, keeping 23588 word types\n",
      "2018-02-26 17:15:14,001 : INFO : PROGRESS: at sentence #870000, processed 2562490 words, keeping 23695 word types\n",
      "2018-02-26 17:15:14,024 : INFO : PROGRESS: at sentence #880000, processed 2592707 words, keeping 23811 word types\n",
      "2018-02-26 17:15:14,043 : INFO : PROGRESS: at sentence #890000, processed 2622712 words, keeping 23921 word types\n",
      "2018-02-26 17:15:14,061 : INFO : PROGRESS: at sentence #900000, processed 2652457 words, keeping 24025 word types\n",
      "2018-02-26 17:15:14,080 : INFO : PROGRESS: at sentence #910000, processed 2682605 words, keeping 24151 word types\n",
      "2018-02-26 17:15:14,099 : INFO : PROGRESS: at sentence #920000, processed 2712598 words, keeping 24257 word types\n",
      "2018-02-26 17:15:14,119 : INFO : PROGRESS: at sentence #930000, processed 2742585 words, keeping 24375 word types\n",
      "2018-02-26 17:15:14,140 : INFO : PROGRESS: at sentence #940000, processed 2772651 words, keeping 24486 word types\n",
      "2018-02-26 17:15:14,161 : INFO : PROGRESS: at sentence #950000, processed 2802773 words, keeping 24592 word types\n",
      "2018-02-26 17:15:14,180 : INFO : PROGRESS: at sentence #960000, processed 2832828 words, keeping 24702 word types\n",
      "2018-02-26 17:15:14,199 : INFO : PROGRESS: at sentence #970000, processed 2863151 words, keeping 24815 word types\n",
      "2018-02-26 17:15:14,221 : INFO : PROGRESS: at sentence #980000, processed 2893385 words, keeping 24922 word types\n",
      "2018-02-26 17:15:14,240 : INFO : PROGRESS: at sentence #990000, processed 2923687 words, keeping 25033 word types\n",
      "2018-02-26 17:15:14,260 : INFO : PROGRESS: at sentence #1000000, processed 2954135 words, keeping 25143 word types\n",
      "2018-02-26 17:15:14,278 : INFO : PROGRESS: at sentence #1010000, processed 2984320 words, keeping 25252 word types\n",
      "2018-02-26 17:15:14,302 : INFO : PROGRESS: at sentence #1020000, processed 3014530 words, keeping 25370 word types\n",
      "2018-02-26 17:15:14,319 : INFO : PROGRESS: at sentence #1030000, processed 3044955 words, keeping 25485 word types\n",
      "2018-02-26 17:15:14,341 : INFO : PROGRESS: at sentence #1040000, processed 3075291 words, keeping 25599 word types\n",
      "2018-02-26 17:15:14,361 : INFO : collected 25699 word types from a corpus of 3101335 raw words and 1048576 sentences\n",
      "2018-02-26 17:15:14,362 : INFO : Loading a fresh vocabulary\n",
      "2018-02-26 17:15:14,410 : INFO : min_count=10 retains 9819 unique words (38% of original 25699, drops 15880)\n",
      "2018-02-26 17:15:14,411 : INFO : min_count=10 leaves 3050364 word corpus (98% of original 3101335, drops 50971)\n",
      "2018-02-26 17:15:14,486 : INFO : deleting the raw counts dictionary of 25699 items\n",
      "2018-02-26 17:15:14,489 : INFO : sample=0.001 downsamples 53 most-common words\n",
      "2018-02-26 17:15:14,490 : INFO : downsampling leaves estimated 2139388 word corpus (70.1% of prior 3050364)\n",
      "2018-02-26 17:15:14,539 : INFO : estimated required memory for 9819 words and 600 dimensions: 52040700 bytes\n",
      "2018-02-26 17:15:14,540 : INFO : resetting layer weights\n",
      "2018-02-26 17:15:14,975 : INFO : training model with 2 workers on 9819 vocabulary and 600 features, using sg=1 hs=0 sample=0.001 negative=5 window=1\n",
      "2018-02-26 17:15:15,997 : INFO : EPOCH 1 - PROGRESS: at 11.11% examples, 234929 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:17,003 : INFO : EPOCH 1 - PROGRESS: at 22.84% examples, 239219 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:18,021 : INFO : EPOCH 1 - PROGRESS: at 35.19% examples, 244478 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:19,044 : INFO : EPOCH 1 - PROGRESS: at 46.87% examples, 244093 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:20,074 : INFO : EPOCH 1 - PROGRESS: at 59.18% examples, 246547 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:21,080 : INFO : EPOCH 1 - PROGRESS: at 70.86% examples, 246893 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:22,101 : INFO : EPOCH 1 - PROGRESS: at 82.56% examples, 247205 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:23,127 : INFO : EPOCH 1 - PROGRESS: at 94.91% examples, 249028 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:23,511 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-26 17:15:23,527 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-26 17:15:23,528 : INFO : EPOCH - 1 : training on 3101335 raw words (2139756 effective words) took 8.5s, 250444 effective words/s\n",
      "2018-02-26 17:15:24,594 : INFO : EPOCH 2 - PROGRESS: at 12.43% examples, 250852 words/s, in_qsize 4, out_qsize 1\n",
      "2018-02-26 17:15:25,615 : INFO : EPOCH 2 - PROGRESS: at 26.10% examples, 265260 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:26,630 : INFO : EPOCH 2 - PROGRESS: at 39.08% examples, 266868 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:27,643 : INFO : EPOCH 2 - PROGRESS: at 50.10% examples, 258296 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:28,675 : INFO : EPOCH 2 - PROGRESS: at 62.77% examples, 259129 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:29,712 : INFO : EPOCH 2 - PROGRESS: at 75.60% examples, 260361 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:30,723 : INFO : EPOCH 2 - PROGRESS: at 87.65% examples, 259991 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:31,744 : INFO : EPOCH 2 - PROGRESS: at 99.00% examples, 257924 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:31,796 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-26 17:15:31,801 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-26 17:15:31,802 : INFO : EPOCH - 2 : training on 3101335 raw words (2139130 effective words) took 8.3s, 258753 effective words/s\n",
      "2018-02-26 17:15:32,838 : INFO : EPOCH 3 - PROGRESS: at 12.75% examples, 264773 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:33,841 : INFO : EPOCH 3 - PROGRESS: at 26.10% examples, 271853 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:34,856 : INFO : EPOCH 3 - PROGRESS: at 39.41% examples, 273488 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:35,879 : INFO : EPOCH 3 - PROGRESS: at 51.40% examples, 267581 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:36,890 : INFO : EPOCH 3 - PROGRESS: at 63.42% examples, 264986 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:37,915 : INFO : EPOCH 3 - PROGRESS: at 74.65% examples, 260048 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:38,943 : INFO : EPOCH 3 - PROGRESS: at 86.69% examples, 259150 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:39,955 : INFO : EPOCH 3 - PROGRESS: at 99.63% examples, 261716 words/s, in_qsize 2, out_qsize 0\n",
      "2018-02-26 17:15:39,964 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-26 17:15:39,984 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-26 17:15:39,985 : INFO : EPOCH - 3 : training on 3101335 raw words (2139675 effective words) took 8.2s, 261749 effective words/s\n",
      "2018-02-26 17:15:41,002 : INFO : EPOCH 4 - PROGRESS: at 12.43% examples, 264039 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:42,016 : INFO : EPOCH 4 - PROGRESS: at 25.12% examples, 262934 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:43,017 : INFO : EPOCH 4 - PROGRESS: at 38.44% examples, 268828 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:44,044 : INFO : EPOCH 4 - PROGRESS: at 50.43% examples, 263647 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:45,070 : INFO : EPOCH 4 - PROGRESS: at 62.77% examples, 262356 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:46,081 : INFO : EPOCH 4 - PROGRESS: at 76.23% examples, 266445 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:47,112 : INFO : EPOCH 4 - PROGRESS: at 88.92% examples, 266399 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:48,115 : INFO : EPOCH 4 - PROGRESS: at 99.95% examples, 263302 words/s, in_qsize 1, out_qsize 1\n",
      "2018-02-26 17:15:48,116 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-26 17:15:48,118 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-26 17:15:48,120 : INFO : EPOCH - 4 : training on 3101335 raw words (2138540 effective words) took 8.1s, 263289 effective words/s\n",
      "2018-02-26 17:15:49,135 : INFO : EPOCH 5 - PROGRESS: at 13.08% examples, 277604 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:50,154 : INFO : EPOCH 5 - PROGRESS: at 26.75% examples, 279027 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:51,163 : INFO : EPOCH 5 - PROGRESS: at 39.41% examples, 274305 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:52,175 : INFO : EPOCH 5 - PROGRESS: at 53.02% examples, 277470 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:53,183 : INFO : EPOCH 5 - PROGRESS: at 64.39% examples, 270348 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:54,183 : INFO : EPOCH 5 - PROGRESS: at 77.18% examples, 271227 words/s, in_qsize 3, out_qsize 0\n",
      "2018-02-26 17:15:55,185 : INFO : EPOCH 5 - PROGRESS: at 90.18% examples, 272688 words/s, in_qsize 4, out_qsize 0\n",
      "2018-02-26 17:15:55,933 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-02-26 17:15:55,955 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-02-26 17:15:55,956 : INFO : EPOCH - 5 : training on 3101335 raw words (2139613 effective words) took 7.8s, 273362 effective words/s\n",
      "2018-02-26 17:15:55,957 : INFO : training on a 15506675 raw words (10696714 effective words) took 41.0s, 261019 effective words/s\n",
      "2018-02-26 17:15:55,958 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-02-26 17:15:56,215 : INFO : saving Word2Vec object under 600features_10minwords_1windows_1skipgram, separately None\n",
      "2018-02-26 17:15:56,216 : INFO : not storing attribute vectors_norm\n",
      "2018-02-26 17:15:56,218 : INFO : not storing attribute cum_table\n",
      "2018-02-26 17:15:57,301 : INFO : saved 600features_10minwords_1windows_1skipgram\n"
     ]
    }
   ],
   "source": [
    "print('Training model...')\n",
    "model = word2vec.Word2Vec(tags_groups, workers = num_workers,\\\n",
    "                         size = num_features, min_count = min_word_count,\\\n",
    "                         window = context, sample = downsampling, sg = skipgram)\n",
    "model.init_sims(replace = True)\n",
    "model_name = str(num_features)+'features_'+str(min_word_count)+ \\\n",
    "                'minwords_'+str(context)+'windows_'+str(skipgram)+'skipgram'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checksimilar(tag,similar,tag_category):\n",
    "    \n",
    "    tag_cate = tag_category[tag]\n",
    "    simi_cate = tag_category[similar]\n",
    "    #make sure none of them has null category\n",
    "    \n",
    "    #import similar groups\n",
    "    similarGroups = []\n",
    "    f = open('categoryGroup.txt',encoding = 'utf8')\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        similargroup = line.strip().split()\n",
    "        similarGroups.append(similargroup)\n",
    "    \n",
    "    #construct taboo list that contains tags that are too general to be compared\n",
    "    tabooList1 = ['database','file','class','function','server','algorithm',\\\n",
    "             'performance','exception','events','video','image','networking',\\\n",
    "             'browser','memory','graph','download','reference','path','transactions',\\\n",
    "             'sdk','camera','datatables','framework','frameworks','operating-system',\\\n",
    "             'ide','format','terminology','applet','controls','return','sql-update',\\\n",
    "             'documentation','embedded','label','closures','iterations','client',\\\n",
    "             'cloud','components','character','ms-office','text-files','prepared-statement',\\\n",
    "             'android-manifest','version','overflow','screen','programming-languages',\\\n",
    "             'keyboard-shortcuts','open-source','accessibility','instance','distinct',\\\n",
    "             'clone', 'customization','overloading','limit','state','monitoring',\\\n",
    "             'native', 'packages', 'transition','center','game-engine','device',\\\n",
    "             'repeater', 'apply','web-api','microcontroller','declaration',\\\n",
    "             'add-in', 'libraries','remote-access','resultset','patch','intervals',\\\n",
    "             'hybrid-mobile-app','instruments', 'actor', 'version', 'multilingual',\\\n",
    "             'point','delete-file', 'keyboard-events', 'analysis','static-analysis',\\\n",
    "             'pseudocode', 'using', 'crash-reports', 'exists', 'absolute', 'multilanguage',\\\n",
    "             'filesize', 'software-design','mobile-application','production', 'dimensions',\\\n",
    "             '2d-games','custom-data-attribute', 'temporary-files', 'material',\\\n",
    "             'agent','file-type','test-case','class-method', 'in-memory-database', \\\n",
    "             'toolkit','jquery-tools', 'github-for-windows', 'collaboration', \\\n",
    "             'review']\n",
    "    tabooList2 = categorylist #to be imported\n",
    "    tabooList = tabooList1 + tabooList2\n",
    "    \n",
    "    similar = False\n",
    "    #check if they are similar tags\n",
    "    if tag_cate == 'null' or simi_cate == 'null' :\n",
    "        similar = False\n",
    "    #compare their categories\n",
    "    elif tag_cate == simi_cate:\n",
    "        similar = True\n",
    "    else:\n",
    "        for group in similarGroups:\n",
    "            if tag_cate in group and simi_cate in group:\n",
    "                similar = True\n",
    "                break\n",
    "\n",
    "    if similar:\n",
    "        #if they are similar, check whether they are taboo words（too general to compare),or they have length of less than 3\n",
    "        for k in [tag,similar]:\n",
    "            #check k, delete tag pair who have tags of length 1 or 2\n",
    "            #delete tag pair who have key word  'database', 'file', 'class','server','function',...(these are too general to be compared)\n",
    "            if len(k) < 3:\n",
    "                similar = False\n",
    "                break\n",
    "            if k in tabooList:\n",
    "                similar = False\n",
    "                break\n",
    "                \n",
    "    if not similar:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-26 17:44:48,225 : INFO : loading Word2Vec object from 600features_10minwords_1windows_1skipgram\n",
      "2018-02-26 17:44:48,692 : INFO : loading wv recursively from 600features_10minwords_1windows_1skipgram.wv.* with mmap=None\n",
      "2018-02-26 17:44:48,694 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-02-26 17:44:48,695 : INFO : loading vocabulary recursively from 600features_10minwords_1windows_1skipgram.vocabulary.* with mmap=None\n",
      "2018-02-26 17:44:48,696 : INFO : loading trainables recursively from 600features_10minwords_1windows_1skipgram.trainables.* with mmap=None\n",
      "2018-02-26 17:44:48,698 : INFO : setting ignored attribute cum_table to None\n",
      "2018-02-26 17:44:48,699 : INFO : loaded 600features_10minwords_1windows_1skipgram\n"
     ]
    }
   ],
   "source": [
    "#def ExamineModel(model_name, most_similar_topn,min_similarity):\n",
    "\n",
    "model = word2vec.Word2Vec.load('600features_10minwords_1windows_1skipgram')\n",
    "\n",
    "#import tag_category list for checksimilar() function\n",
    "tag_category = []\n",
    "#to be passed into pandas\n",
    "location_tag_cate = 'Tag_Category.csv'\n",
    "#dataframe that contains tags for each post\n",
    "df = pd.read_csv(location_tag_cate,names = ['Tag','Category'])\n",
    "for index,row in df.iterrows():\n",
    "    tag_category[row['Tag']] = row['Category']\n",
    "\n",
    "tag_frequency_array = []\n",
    "\n",
    "#build an array according to tags' frequency\n",
    "for key,value in model.wv.vocab.items():\n",
    "    count = value.count\n",
    "    new_tuple = (key,count) #python, its_frequency\n",
    "    tag_frequency_array.append(new_tuple)\n",
    "    \n",
    "#sort the array according to tags' frequency\n",
    "dtype = [('tag','S50'),('frequency',int)]\n",
    "array_bf_sort = np.array(tag_frequency_array,dtype = dtype)\n",
    "tag_frequency_sorted = np.sort(array_bf_sort, order = 'frequency')[::-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c#', 'vb.net', 'invalidoperationexception', 'dispose', '.net', 'nullreferenceexception', 'f#', 'linq-to-twitter', 'portable-class-library', 'aspxgridview', 'compact-framework', 'async-await', 'objectdisposedexception', 'wcf-ria-services', 'windows-applications', 'databound', 'notsupportedexception', 'xmlserializer', 'domainservices', 'local-database', 'assembly-references', 'openrasta', 'datapager', 'site.master', 'objectdatasource', 'bindinglist', 'sqlmetal', 'datacontractserializer', '.net-cf', 'datacontext', 'linq-to-objects', 'streamwriter', 'application-settings', 'semantic-zoom', 'infragistics', 'argumentexception', 'dotfuscator', '.net-assembly', 'reactive-programming', 'propertychanged', 'full-trust', '.net-framework-version', 'ado.net-entity-data-model', 'silverlight-oob', 'visual-web-developer', 'linq-to-sharepoint', 'webclient-download', 'idataerrorinfo', 'class-library', 'medium-trust', 'currentuiculture', 'contentcontrol', 'nancy', 'linq-to-sql', 'strong-typing', 'wcf-data-services', 'dynamic-linq', 'streamreader', 'linqkit', 'azure-storage-tables', 'linqpad']\n",
      "['visual-studio', 'visual-studio-2008-sp', 'visual-studio-express', 'visual-studio-2010-sp', 'visual-studio-debugging', 'visual-studio-2010-beta', '.net-assembly', 'projects-and-solutions', 'visual-c#-express', 'visual-web-developer', 'intellisense', 'visual-studio-extensions', 'vspackage', 'windbg', 'c++-cli', 'vc', 'ca', 'resharper-plugins', 'resharper', 'assembly-references', 'sandcastle', 'clr', 'solution']\n",
      "['html', 'opera', 'cross-browser', 'safari', 'mobile-safari', 'w3c', 'modernizr', 'browser-feature-detection', 'fancybox', 'progressive-enhancement', 'mootools', 'firefo', 'polyfills', 'jplayer', 'dojo', 'webkit', 'google-chrome', 'html5-filesystem', 'onsubmit', 'fabricjs', 'dom', 'html5-audio', 'mozilla', 'mobile-website', 'src', 'google-chrome-devtools', 'mobile-webkit', 'fileapi', 'accessibility', 'browser-detection', 'dart-polymer', 'data-url', 'web-standards', 'section', 'wcag', 'webshim', 'mobile-browser', 'opera-mini', 'meta-tags', 'same-origin-policy', 'markup', 'iscroll', 'shadow-dom', 'onload-event', 'innerhtml', 'arraybuffer', 'jquery-mobile-listview', 'onclick', 'html-form', 'javascript-events', 'todataurl', 'jquery-cycle', 'image-resizing', 'qtip', 'firefox', 'jcanvas', 'data-uri', 'html5-canvas', 'raphael', 'soundmanage', 'html5-animation']\n",
      "['asp.net-mvc', 'webforms', 'webmatrix', 'mvccontrib-grid', 'mvc-editor-templates', 'asp.net-webpages', 'viewdata', 'asp.net-mvc-partialview', 'global-asax', 'portable-areas', 'html.beginform', 'modelstate', 'mvcsitemap', 'html.actionlink', 'custom-membershipprovider', 'asp.net-ajax', 'actionmethod', 'telerik-mvc', 'defaultmodelbinder', 'attributerouting', 'asp.net-mvc-controller', 'asp.net-mvc-views', 'web-api', 'custom-action-filter', 'asp.net-mvc-viewmodel', 'nopcommerce', 'system.web.optimization', 'asp.net-mvc-3-areas', 'asp.net-mvc-routing', 'asp.net-mvc-areas', 'sitefinity', 'telerik', 'httppostedfilebase', 'signalr-hub', 'html.dropdownlistfor', 'webforms-view-engine', 'asp.net-routing', 'selectlistitem', 'actionlink', 'custom-model-binder', 'kendo-grid', 'action-filter', 'asp.net-membership', 'asp.net-web-api-routing', 'telerik-grid', 'unobtrusive-validation', 'viewbag', 'modelmetadataprovider', 'generic-handler', 'httphandler', 'razor-declarative-helpers', 'razor', 'ravendb', 'scriptmanager', 'umbraco', 'nancy', 'createuserwizard', 't4mvc', 'postback', 'html-helper', 'dotnetnuke']\n",
      "['ruby-on-rails', 'railscasts', 'activeadmin', 'ruby-on-rails-plugins', 'gem', 'rails', 'rails-engines', 'sinatra', 'rubygems', 'rake', 'delayed-job', 'refinerycms', 'railstutorial.org', 'paperclip', 'strong-parameters', 'devise', 'padrino', 'spree', 'erb', 'rspec', 'nomethoderror', 'actionview', 'ruby', 'haml', 'form-for', 'carrierwave', 'rails-activerecord', 'uninitialized-constant', 'rails-routing', 'rails-console', 'mongoid', 'formtastic', 'simple-form', 'rails-admin', 'rescue', 'nested-attributes', 'activemodel', 'bundler', 'nested-resources', 'rack', 'cancan', 'actionviewhelper', 'rails-3-upgrade', 'authlogic', 'nested-forms', 'activesupport', 'sti', 'globalize', 'actioncontroller', 'fields-for', 'webrick', 'attr-accessible', 'link-to', 'will-paginate', 'rails-models', 'faye', 'respond-to', 'acts-as-list', 'ransack', 'kaminari', 'resque']\n"
     ]
    }
   ],
   "source": [
    "most_similar_topn = 60 #\n",
    "min_similarity = 0.6 #\n",
    "for tag_freq in tag_frequency_sorted[:5]:\n",
    "    tag = str(tag_freq[0],'utf-8')\n",
    "    most_similar_60 = model.wv.most_similar(tag,topn = most_similar_topn)\n",
    "    tag_similar_list = []\n",
    "    tag_similar_list.append(tag)\n",
    "    for word in most_similar_60:\n",
    "        if word[1]>min_similarity:\n",
    "            tag_similar_list.append(word[0])\n",
    "    print(tag_similar_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
